{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Some basic Tensorflow NLP APIs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4HNKDfiTRVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozDf7l7-TfYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRX-d_SLTk1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "'I love my dog',   \n",
        "'I love my cat',\n",
        "'You love my dog',\n",
        "' Do you think my cat is amazing',\n",
        "'I have a new car'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AESa5KVrTvjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENqI0sCEUADG",
        "colab_type": "text"
      },
      "source": [
        "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZE4NqhHUJ0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating an instance of a tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ci457EyUYoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calling the fit method which takes in the data and encodes it\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVf0z6A5VETZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_HkpGS_VMum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11646b3a-729c-43f6-fe75-626367e7561f"
      },
      "source": [
        "# Note that the Tokenizer strips out the punctuation\n",
        "\n",
        "print (word_index)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'i': 2, 'love': 3, 'dog': 4, 'cat': 5, 'you': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10, 'have': 11, 'a': 12, 'new': 13, 'car': 14}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJvZhO8nVOja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA6L7N4LXNW-",
        "colab_type": "text"
      },
      "source": [
        "**Text to Sequence**\n",
        "\n",
        "Now our goal is to turn our sentences into lists of values based on the tokens that we have created above.\n",
        "We also face the difficulty to also manipulate these lists including but not limited to making each list the same length. Otherwise it may be hard to train a neural network with them "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqdjqiX7XLYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OIzGzIYXLd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eae04743-51c5-439a-c708-8da9b7925d25"
      },
      "source": [
        "print(sequences)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 1, 4], [2, 3, 1, 5], [6, 3, 1, 4], [7, 6, 8, 1, 5, 9, 10], [2, 11, 12, 13, 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0mPswd_XLi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vBAGD5mYkWp",
        "colab_type": "text"
      },
      "source": [
        "**Let us do inference from this model**\n",
        "One thing to keep in mind while doing inference from these models is that the words or sentences you are using as test should be encoded in the same word index. Otherwise, the result would be meaningless"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTYj631XLlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sequence = [\n",
        "    \n",
        "    'i really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "    \n",
        "    \n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqDcvr53XLqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sequence = tokenizer.texts_to_sequences(test_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBfPoiq7XLtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96f9ba28-6691-497d-ea33-81c760f8017e"
      },
      "source": [
        "print (test_sequence)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 1, 4], [1, 4, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl_BkIyRXLvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdU3_zWtfHZM",
        "colab_type": "text"
      },
      "source": [
        "Efficiency Tip:\n",
        "Use a special character or number for a word that is encountered during your test data set, instead of leaving it as it is. We can do it in Keras very easily.\n",
        "Let's rewrite the \"tokenizer \" in a different way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwJldL1OXLyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1a6a9d5e-034f-4bb6-a2b9-42cf12c9228f"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100, oov_token = ' <OOV>')\n",
        "\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print (word_index)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' <OOV>': 1, 'my': 2, 'i': 3, 'love': 4, 'dog': 5, 'cat': 6, 'you': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11, 'have': 12, 'a': 13, 'new': 14, 'car': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyNau8FOXL06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sequence = [\n",
        "    \n",
        "    'i really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "    \n",
        "    \n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUt8CMh-XL3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sequence = tokenizer.texts_to_sequences(test_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEeVavAJXL56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fea604f-09e8-4fd0-89cb-a3714a15b1ec"
      },
      "source": [
        "print (test_sequence)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 1, 4, 2, 5], [2, 5, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qif2Up6lXL8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpuXbElwgXW2",
        "colab_type": "text"
      },
      "source": [
        "**Padding**\n",
        "As we mentioned earlier , it is important for a neural network to be trained on data that is uniform in size but this is not always the case. To deal with issue, we use padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or9EL1dDXL_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First , import the corresponding library from tensorflow. \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iezuJMWNXMEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad the sequences\n",
        "padded = pad_sequences(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_AV4RudXMJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e4e5c4c5-ac11-4c06-b7bb-b45714aadf07"
      },
      "source": [
        "print (\"Sequences are \" + str(sequences))\n",
        "print (\"Padded Sequences are \"+ str(padded))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequences are [[3, 4, 2, 5], [3, 4, 2, 6], [7, 4, 2, 5], [8, 7, 9, 2, 6, 10, 11], [3, 12, 13, 14, 15]]\n",
            "Padded Sequences are [[ 0  0  0  3  4  2  5]\n",
            " [ 0  0  0  3  4  2  6]\n",
            " [ 0  0  0  7  4  2  5]\n",
            " [ 8  7  9  2  6 10 11]\n",
            " [ 0  0  3 12 13 14 15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OoEBMz4l95-",
        "colab_type": "text"
      },
      "source": [
        "**If we want to pad the sentences after the text , we can make a little tweak to the above code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ray5CIylsCz",
        "colab_type": "text"
      },
      "source": [
        "**As we see , all of the sequences have lenght equal to the lenght of the longest sequence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4zvmO74XMMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4549d6d1-2278-4a40-9d77-b1ae3cb1d164"
      },
      "source": [
        "padded = pad_sequences(sequences , padding = 'post')\n",
        "print (\"Sequences are \" + str(sequences))\n",
        "print (\"Padded Sequences are \"+ str(padded))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequences are [[3, 4, 2, 5], [3, 4, 2, 6], [7, 4, 2, 5], [8, 7, 9, 2, 6, 10, 11], [3, 12, 13, 14, 15]]\n",
            "Padded Sequences are [[ 3  4  2  5  0  0  0]\n",
            " [ 3  4  2  6  0  0  0]\n",
            " [ 7  4  2  5  0  0  0]\n",
            " [ 8  7  9  2  6 10 11]\n",
            " [ 3 12 13 14 15  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HaUgRpcmTfT",
        "colab_type": "text"
      },
      "source": [
        "We can set the max lenght of any sentence by the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Aw2h5-XMOh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "22e06ba1-51d0-4adf-b8e9-c217ac655ce2"
      },
      "source": [
        "padded = pad_sequences(sequences , padding = 'post' , maxlen = 5)\n",
        "print (\"Sequences are \" + str(sequences))\n",
        "print (\"Padded Sequences are \"+ str(padded))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequences are [[3, 4, 2, 5], [3, 4, 2, 6], [7, 4, 2, 5], [8, 7, 9, 2, 6, 10, 11], [3, 12, 13, 14, 15]]\n",
            "Padded Sequences are [[ 3  4  2  5  0]\n",
            " [ 3  4  2  6  0]\n",
            " [ 7  4  2  5  0]\n",
            " [ 9  2  6 10 11]\n",
            " [ 3 12 13 14 15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o46H2Yi7mkW7",
        "colab_type": "text"
      },
      "source": [
        "To avoid using the information from the start of the sentence using \"**maxlen**\" constraint , we can use another way around it and set the variable **truncating** to be post. In this way , we lose the information from the end of the sentence rather than from the front "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvADNQb1mNT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "72eea64a-36ce-4ece-ca43-2bd20c66333c"
      },
      "source": [
        "padded = pad_sequences(sequences , padding = 'post' , maxlen = 5 , truncating = 'post')\n",
        "print (\"Sequences are \" + str(sequences))\n",
        "print (\"Padded Sequences are \"+ str(padded))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequences are [[3, 4, 2, 5], [3, 4, 2, 6], [7, 4, 2, 5], [8, 7, 9, 2, 6, 10, 11], [3, 12, 13, 14, 15]]\n",
            "Padded Sequences are [[ 3  4  2  5  0]\n",
            " [ 3  4  2  6  0]\n",
            " [ 7  4  2  5  0]\n",
            " [ 8  7  9  2  6]\n",
            " [ 3 12 13 14 15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRTJG1dNmNWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIHYuaQVmNY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBRoF_mYmNbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7xuyMCYmNeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgBgaKGfmNg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x425EPe2mNju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9PbmbpqmNmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fide2fhQmNpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWo9HMsUmNsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW6wMjlCmNur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLlEKFUTmNz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BNGb8ZEmN6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8LzFewDmN-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}